# Abstract

- This document explores design systems and services at Facebook Scale

# Preparation - General QA

## What are design questions?

1. High level architecture for some sort of a software system. Can be a web facing service, a RESTful API, P2P desktop app.
2. Examples Questions
   1. Design a URL shortening service like bit.ly
   2. How would you implement Google Search?
   3. Design a client-server application which allows people to play chess with one another
   4. How would you store relations in a social network like Facebook and implement a feature where one user receives notifications when their friends like the same things as they do.
3. Tips
   1. The idea is to have a discussion about the problem at hand.\*\*\*\*
   2. Focus on the way you tackle the problem.
   3. Discuss high-level architecture addressing the goals and constraints in the question.
   4. Focus on a combination of right strategy and knowledge
      1. Strategy => A way to approach the problem at an interview by focusing on the right thing

## System Design Process

1. Constraints and use cases

   1. Use Cases:
      1. Define all possible use cases verify with interviewer for required use cases. Then narrow the scope.
   2. Constraints
      1. Constraints number may be given by the interviewer, and if not and the interviewer gives a vague number such as top 10 company in this software service sector.
      2. Calculate monthly request, request per second, network data flow per seconds, and future storage requirement for 5 years to satisfy all above use cases and constraints as being a top 10 company.
      3. Use these data to better define the problem constraints and scope.
      4. 1 Day => 24 \* 3600 seconds => 86,400 seconds
      5. 1 Month => 86,400 \* 30 => 2,592,000 seconds
   3. Evaluation
      1. You will be evaluated based on how you gather requirements about the problem at hand and how you design a solution that covers those requirements well.
   4. Do & Don't
      1. Don't design solution to the problem immediately when it is weakly defined
      2. Do clarify the system's constraints and identify use cases the system needs to satisfy.
      3. Agree on the scope of the system. Never assume, be explicit.

2. Abstract Design

   1. Design Outline
      1. Outline all the important components that your architecture will need.
      2. Sketch main components and the connections between them and gain feedback from the interviewer. Justify your ideas in front of the interviewer and try to address every constraint and use case.
      3. Provide rudimentary implementations.
   2. Do & Don't
      1. Don't get lured to dive deep into some particular aspect of the abstract design.

3. Understand Bottlenecks

   1. Thinking about bottlenecks
      1. Can the system handle all the requests from the user? If not, should we introduce load balancer or in memory database.
      2. Is the data so huge that you need to distribute your database on multiple machines? What are the downsides? Is the db too slow or does it need in-memory cache?
      3. Even if you don't have to address all the bottlenecks, you need to be able to identify weak spots in a system and be able to resolve them.
   2. Tips
      1. Usually each solution is a trade-off of some kind. Changing something will worsen something else. However, the important thing is **to be able to talk about these trade-offs**, and to **measure their impact on the system** given the constraints and use cases defined.

4. Scaling your abstract design

   1. Thinking about scaling
      1. Once bottlenecks are identified, start reasoning about how to resolve those bottlenecks to scale your design.

5. Summary - system design process
   1. **Scope** the problem. Make no assumptions and ask questions. Understand the constraints and uses cases
   2. Sketch up an **abstract design** that illustrates the basic components of the system and the relationships between them.
   3. Think about the **bottlenecks** these components face when the system scales.
   4. Address these bottlenecks by using the fundamentals principles of **scalable system design**.

### Scalability

1. Topics
   1. Vertical Scaling, Horizontal Scaling, Caching, Load balancing, Database replication and partitioning.
      1. Consider NoSQL and sharding when thinking about scaling relational database and be asynchronous.
2. Sharding
   1. Advantages:
      1. High Availability => If one box goes down the others still operate
      2. Faster Queries => Smaller amounts of data in each user group means faster querying
      3. More write bandwidth => No master db serializing writes you can write in parallel which increases your write throughput
      4. You can do more work => A parallel backend means doing multiple jobs simultaneously.
   2. Differences:
      1. Data are denormalized => Traditionally we normalize data. Now it's sprayed out into anomaly-less tables and then joined back together again when they need to be used. **You store together data that are used together**.
      2. Data are parallelized across many physical instances => DBs are historically vertical scaling, scale up. Sharding allows data to be parallelized and scaled out.
      3. Data are kept small => The larger a set of data a server handles the harder it is to cash intelligently because you have such a wide diversity of data being accessed. It allows isolation of data into smaller shards and data is more likely to stay in cache.
      4. Data are more highly available => Since the shards are independent a failure in one doesn't cause a failure in another.
      5. It doesn't use replication => Replicating data from a master server to slave servers is a traditional approach to scaling. Here the master is the write bottleneck and sharding solves this problem.
   3. Problems:
      1. Rebalancing data:
         1. What happens when a shard outgrows your storage and needs to be split. Your need to move the data to a different shard.
      2. Joining data from multiple shards:
         1. You have to query multiple shards and build a page out of all data that's being sent back. Can be fast because of caching and fast networks
      3. How do you partition your data in shards?
         1. No easy answers.
      4. Less leverage
         1. No support tools and no IDE help. You are on your own when doing sharding.
      5. Implementing shards is not well supported
         1. No implementation support as tool chains are not available. Features are still under development.
         2. MySQL has partitioning support provided by InnoDB storage engine.

# Topics - Solutions

1. How to design Pastebin.com (How to design a URL shortening service like bit.ly)

   1. Calculate usage - ask if you should run back-of-the-envelope usage calculation
      1. Size per paste - 1.27KB
         1. 1 KB content per paste, 7 bytes shortlink, 4 bytes expiration_length_in_minute, 5 bytes created_at, paste_path - 255 bytes => total ~1.27KB
      2. 12.7GB of new paste content per month
         1. 1.27 KB per paste \* 10 million pastes per month => 12.7GB per month
         2. 360 million shortlinks in 3 years
         3. ~450 GB of new paste content in 3 years
         4. assume most are new pastes instead of updates to existing ones
         5. 4 paste writes per second on average
         6. 40 read requests per second on average
   2. Design Core Components
      1. Database Design, primary key, indexes length...
      2. write api algorithm, MD5 for uniform distributed hashing, Base62 to encode the result string.
      3. web server api
         1. Request, curl -X POST --data '{ "expiration_length_in_minutes": "60", "paste_contents": "Hello World!" }' https://pastebin.com/api/v1/paste
         2. Response, { "shortlink": "foobar" }
         3. Request, curl https://pastebin.com/api/v1/paste?shortlink=foobar
         4. Response, { "paste_contents": "Hello World" "created_at": "YYYY-MM-DD HH:MM:SS" "expiration_length_in_minutes": "60" }
   3. InfraDesign
      1. Client requests CDN which routes to object store to quicken object delivery
      2. Load balance to web server to distribute load
      3. Web server as load balancer to distribute requests to read api or write api for different request types
      4. Read API reads from memory cache, sql database read replicas and object store
      5. Write API writes to SQL database, replicated through master-slave, and object store
      6. Analytics read from object store and SQL analytics
         1. MapReduce web server logs to generate hit counts.
         2. For each log line, extract url, year and month so it can be used to reduce the count of url hit during a specific date on a specific url.

2. How to design the Twitter timeline and search

   1. Use Cases
      1. User post a tweet; service pushes tweet to followers, sending push notifications and emails
      2. User views the user timeline (activity from the user)
      3. User views the home timeline (activity from people the user is following)
      4. User searches keywords
      5. Service has high availability
   2. Out of scope
      1. Service pushes tweets to the Twitter Firehose and other streams
      2. Service strips out tweet based on user's visibility settings.
         1. Hide @Reply if the user is not also following the person being replied to
         2. Respect 'hide retweets' setting
      3. Analytics
   3. Constraints and Assumptions
      1. State assumptions
         1. Traffic is not evenly distributed
         2. Posting a tweet should be fast and fanning out a tweet to all of your followers should be fast, unless you have millions of followers
         3. 100 million active users
         4. 500 million tweets per day, 15 billion tweets per month
            1. Each tweet averages a fanout of 10 deliveries
               1. 5 billion total tweets delivered on fanout per day
               2. 150 billion tweets delivered on fanout per month
         5. 250 billion read requests per month
         6. 10 billion searches per month
      2. Timeline
         1. Viewing the timeline should be fast
         2. Twitter is more read heavy than write heavy
            1. Optimize for fast reads of tweets
         3. Ingesting tweets is write heavy
      3. Search
         1. Searching should be fast
         2. Search is read-heavy
   4. Calculation - calculate write first storage first
      1. Storage
         1. Tweet size - 10 KB
            1. tweet_id 8 bytes, user_id 32 bytes, text 140 bytes, media 10KB average
         2. New tweet content per month - 150 TB
            1. 10 KB x 500 million tweet x 30 days => 150 TB
            2. 12 x 3 x 150 TB => 5.4 PB in 3 year
      2. Networking traffic
         1. Read requests per second - 100K
            1. 250 billion read requests per month \* (400 requests per second / 1 billion requests per month)
         2. 6,000 write tweets per second
            1. 15 billion tweets per month \* (400 requests per second / 1 billion requests per month)
            2. 15 billion / 30 /24/ 3600
         3. 60 thousand tweets delivered on fanout per second
            1. 150 billion tweets delivered on fanout per month \* (400 requests per second / 1 billion requests per month)
            2. 150 billion / 30 /24/ 3600
         4. 4,000 search requests per second, 10 billion searches / 2.6 million seconds ~= 4,000 searches
   5. High Level Design
      1. Client communicate with web server that interacts with write api, read api, and search api
         1. Read api
            1. Reads from MySQL and timeline service, which itself has access to tweet info service, user info service and memory cache.
            2. Read API has access to MySQL data in case there is a cache miss from timeline service which constructs timeline data from cache and other meta data service.
         2. Write api
            1. Passes tweet data to fanout service which itself fanout data to memory cache, user graph service, search service, notification service, object store and MySQL database.
         3. Search api
            1. It uses search service to get relevant search results.
   6. Core Components
      1. Storage decision
         1. Delivering tweets and building the home timeline is hard problem.
         2. Fanning out to all followers 60K tweets per second will overload RDBMS
         3. Choose a data store with fast writes such as NoSQL or Memory Cache
            1. Reading 1MB from memory sequentially takes about 250 microseconds, while reading from SSD is 4x (1 milliseconds) and disk takes 80x (20 milliseconds)
         4. **Media such as photos or videos should be stored on an Object Store**
      2. Tweet post design
         1. **Client posts a tweet to the Web Server, running as a reverse proxy, forwarding requests to write api server**
         2. **write api stores the tweet in the user's timeline on a SQL database**
         3. **write api contacts the fan out service, which does the following**
            1. **Queries the User Graph Service to find the user's followers stored in the Memory Cache**
            2. **Stores the tweet in the home timeline of the user's followers in a Memory Cache**
               1. **O(n) operations 1,000 followers = 1,000 lookups and inserts**
            3. **Stores the tweet in the search index service to enable fast searching**
            4. **Stores media in the Object Store**
            5. **Uses the Notification Service to send out push notifications to followers**
               1. **This service can itself have a queue and further decouple & scale its messaging capabilities**
      3. Memory Cache is Redis
         1. Store Tweets as **Redis list structure with, tweet n => tweet_id 8 bytes, user_id 8 bytes, meta 1 byte**
      4. Use Cases Implementation walk-through
         1. User post a tweet; service pushes tweet to followers, sending push notifications and emails
            1. REST API
               1. curl -X POST --data '{ "user_id": "123", "auth_token": "ABC123", \ "status": "hello world!", "media_ids": "ABC987" }' \ https://twitter.com/api/v1/tweet
               2. { "created_at": "Wed Sep 05 00:37:15 +0000 2012", "status": "hello world!", "tweet_id": "987", "user_id": "123", ... }
         2. User views the user timeline (activity from the user)
            1. Flow
               1. The client posts a user timeline request to the Web Server
               2. The Web Server forwards the request to the READ API Server
               3. **The Read API retrieves the user timeline from the SQL database**
         3. User views the home timeline (activity from people the user is following)
            1. Flow
               1. The client posts a home timeline request to the web server
               2. The web server forwards the request to the Read API server
               3. **The Read API server contacts the Timeline Service, which does the following:**
                  1. **Gets the timeline data stored in Memory Cache, containing tweet ids and user ids - O(1)**
                  2. **Queries the Tweet Info Service with a multiget to obtain additional info about the tweet ids - O(n)**
                  3. **Queries User Info Service with a multiget to obtain additional info about user ids - O(n)**
            2. REST API
               1. curl https://twitter.com/api/v1/home_timeline?user_id=123
               2. [{ "user_id": "456", "tweet_id": "123", "status": "foo" }, { "user_id": "789", "tweet_id": "456", "status": "bar" }, { "user_id": "789", "tweet_id": "579", "status": "baz" }]
         4. User searches keywords
            1. Client sends a search request to the Web Server
            2. Web Server forwards the request to search API server
            3. **Search API contacts the service which does the following**
               1. **Parse/tokenizes the input query, determining what needs to be searched**
                  1. Remove markup, breaks up the text into terms, fixes typos, normalize capitalization, converts the query to use boolean operations
               2. **Queries the Search Cluster (ie Lucence) for the results**
                  1. **Scatter gathers each server in the cluster to determine if there are any results for the query**
                  2. **Merge, ranks, sorts, and returns the results**
         5. Service has high availability
            1. REST API
               1. curl https://twitter.com/api/v1/search?query=hello+world
   7. Scale the Design
      1. State that you would benchmark/load test, profile for bottlenecks and address them whilte evaluating alternatives and tradeoffs and repeat.

3. How would you implement the Google search?
   1. Indexing Infrastructure
      1. Web crawlers craws the corpus of data and produces the results in a data structure that is more efficient for doing reads.
      2. Crawler
         1. Spider web page links and dump them into a set and avoid getting caught in infinite loop and infinitely generated content. Place each of these links in one massive text file
      3. Indexer
         1. Runs a Map/Reduce job which the map job takes a single web link, retrieve the website, and convert it into an index file and the reduce job will simply aggregate all of these index files into a single unit rather than a million of loose files. Use a farm of servers in a large data center to do this. Classify pages with their words and index ranking based on different factors.
   2. Specifics of Indexing Algorithm
      1. How to compute meaningful results (search index)
         1. For each web site, count the number of incoming links, look at how the link was represented like h1 tag and b tag has more importance than h3
         2. For each web site, look at the number of outbound links, look at the types of words used, like hash, table, probably means sites related to Computer Science. hash and brownies on other hand would imply food.
         3. Create a map of keyword, page, index value for every single page to be used for querying
   3. Serving the Result
      1. Query algorithm maps a query to keywords
      2. Calculate ranks using classified keywords derived from query and its weight. Return pages rank details for the first 50 queries and the rest to be a sum of all pages with keywords related to this search. Asynchronously build the keywords, page, index table and store them in a cache for faster access
      3. If new query comes in with new keywords, quickly calculate first 50 pages with highest 100 index of each page and return user with response. Then asynchronously build a keywords, page, index table and store them for later access.
   4. Core components
      1. Front End web server with logics to parse user query into a standard interface for keyword, weight, intent, etc.., for more precise querying.
      2. Middle tier Caching, if, (keywords..., page, index) table sorted by index is not created, then create one.
      3. Crawler puts data in a queue so that document service store them in a document store.
      4. Reverse Index service can look up data put inside a queue by crawler service and process it.
4. Design a web crawler

   1. Use Case
      1. Service crawls a list of urls
         1. Generates reverse index of words to pages containing the search terms
         2. Generate titles and snippets for pages
            1. Titles and snippets are static and they do not change based on search query
      2. User inputs a search term and sees a list of relevant pages with titles and snippets the crawler generated
      3. Service has high availability
   2. Constraints
      1. Traffic not evenly distributed, some searches are very popular
      2. Support only anonymous users
      3. Generating search results should be fast
      4. the web crawler should get not get stuck in infinite loop
      5. **1 billion links to crawl**
         1. Pages need to be crawled regularly to ensure freshness
         2. Average refresh rate of about once per week, more frequent for popular sites
            1. Therefore **4 billion links** crawled each month
         3. Average store size per web page 500 KB
      6. 100 billion searches per month
      7. Calculate Usage:
         1. 2 PB of stored page content per month
            1. 500 KB per page \* 4 billion links crawled per month
            2. 72 PB of stored page content in 3 years
         2. 1,600 write requests per second
         3. 40,000 search requests per second

5. Design a client-server application which allows people to play chess with one another.

   1. Assumptions
      1. You can have tens of thousands of parallel games playing at the same time
      2. You create a game share it via the game or via an URL invite
      3. You must sign up via email, Facebook or Google OAuth in order to play
      4. Gameplay is asynchronous
      5. Notification system to alert you in-app or via email for your turn to play
      6. Nouns, sessions, players, games
   2. Design
      1. Game
         1. player1, uuid, player2 uuid, last_active(epoch timestamp), move_history (string array)
            1. last_active allows to track the game is active
            2. move_history can be used to infer:
               1. currentTurn, enum => white, black
               2. board, 2-d array of board squares, piece names enum => 'knight', 'bishop', etc..
               3. winState, enum => white,black or null
               4. takenWhitePieces: array[] => piece names
               5. takenBlackPieces: array[] => piece names
         2. Core Actions
            1. UI reacts to user events
            2. A controller layer marshals those user events into abstract moves that are sent as messages to the **game logic component**
            3. The game logic updates move history and thus board state
            4. Controller is notified of update to board state
            5. Controller passes board state to renderer
            6. UI is updated with fresh rendering
         3. Estimation
            1. Assume 30 seconds of think time on average, or 2 moves/min per game
            2. 10,000 games and 2 move per minute per game means 20,000 moves/minute, ~330 moves per second
            3. Say 50 moves per game and takes 8 hours to complete on average.
               1. 500,000 moves distributed over 3600\*8, 28,800 seconds. Or 17 moves per second
         4. Core components
            1. A cluster of web app servers fronted by a load balancer, and a single database to store all the games and sessions. Master-slave replication, read replica needed for scale.
            2. If the browser session-lookup traffic to RDBMS is too much, then store session lookup inside a Redis instance to store the sessions. But for this amount of traffic RDBMS is not a bottleneck.

6. How would you store the relations in a social network like Facebook and implement a feature where one user receives notifications when their friends like the same things as they do?
   1. High Level Design Definitions and Reasoning
      1. # of Relations between users can be seen as a table of user1, user2, connection date. The storage requirement will be number of distinct connections all users have.
         1. On average one user has 100 relations and say 1 billion users, we will have 100 billion relations
      2. There will be following tables
         1. posts
            1. post_id 8 bytes, user_id 8 bytes, post 100 KB, created date
         2. Relations
            1. user_id_to, user_id_to, relation_status, created_date
         3. Likes
            1. post_id 8 bytes, user_id, create_at
      3. Service
         1. When a user clicks like in the website, it sends a like request to the web server with post_id, user_id.
         2. Web server takes that information and synchronously pass it to write api
         3. Write api passes data to fanout service to write to memcache cache likes and inserts the data to SQL database.
         4. Then fanout service queries user graph service for all friends user_id is linked and queries and queries all users in the memcache for the post_id. Then it does a set intersection operation for all user_ids that liked the post that's also this users' friend.
         5. Then fanout service, sends a list of "user_id, post_id, friend_id, create_at" to the notification service which handles notification back to users via email or via asynchronously response.

# Design Tips

1. When to use NoSQL
   1. Rapid ingest of clickstream and log data which never changes
   2. Leaderboard or scoring data
   3. Temporary data, such as shopping cart
   4. Frequently accessed ('hot') tables
   5. Metadata/lookup tables

# Rubrics To Topics

- Ask questions and resolve ambiguity first
- Identify most critical components of the design first, then drive overall design based on that.
- Describe your system design and overall architecture at high level first
- Draw diagrams that CLEARLY describe the RELATIONSHIP among different system components
- Identify and think out loud your reasoning on the trade-offs of your design (**Consistency, availability, partitioning, performance**)
- Calculate resources necessary for your design (Storage, CPU, RAM, Disk type, bandwidth, etc...)
- Plan for system design requirements/constraints change
- Determine how your system will scale and possible bottleneck/limitation of your design
- Explain how it handles both success and failure scenario
